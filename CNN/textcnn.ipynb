{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![G4__4TRKCN_5BDKN_6@6TC3.png](https://i.loli.net/2021/10/09/QLamk34cBFdMNgr.png)\n",
    "\n",
    "这里的feature map 有9行 表示有9个词，有6行 表示每个词用6维来进行编码\n",
    "这里每个词向量是都是用word_embedding来表示\n",
    "\n",
    "在这里面每个卷积核也不一定是相等的，例如第一个红色的卷积核，他的长度是6，宽是2，这样的好处就是可以同时考虑多个词，类似于bigram。卷积核设置为3或4都可以。\n",
    "\n",
    "这里面的结构都很简单，难得是数据处理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一些常规的数据\n",
    "#自己设置的数据集\n",
    "#这里的实现没有用到ngram\n",
    "sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good. 二分类问题\n",
    "\n",
    "# TextCNN Parameter\n",
    "embedding_size = 2 #每个单词要用几维的向量来表示\n",
    "sequence_length = len(sentences[0]) # 规定了每个句子的长度\n",
    "num_classes = len(set(labels))  # 去重之后的长度\n",
    "batch_size = 3\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "vocab = list(set(word_list))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader 调用数据的方式\n",
    "def make_data(sentences, labels):\n",
    "    inputs = []\n",
    "    #下面把输入处理成6*3的矩阵 6句话 每句话3个词\n",
    "    for sen in sentences:\n",
    "        inputs.append([word2idx[n] for n in sen.split()]) \n",
    "    #print(inputs)\n",
    "    targets = []\n",
    "    for out in labels:\n",
    "        targets.append(out)\n",
    "    return inputs,targets\n",
    "    \n",
    "input_batch, target_batch = make_data(sentences,labels)\n",
    "input_batch, target_batch = torch.LongTensor(input_batch),torch.LongTensor(target_batch)\n",
    "\n",
    "dataset = Data.TensorDataset(input_batch, target_batch)\n",
    "loader = Data.DataLoader(dataset, batch_size, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PVCB_`SA00HOZ6I_1YVG_UI.png](https://i.loli.net/2021/10/10/34gdxsGkif9QJ2o.png)\n",
    "\n",
    "输入数据是个矩阵，矩阵维度为 [batch_size, seqence_length]，矩阵的每一行是一个句子，每个数字代表一个词，然后我们把他映射到embedding层上，每个词用一个向量表示，比方说 12 可能会变成 [0.3,0.6,0.12,...]，因此整个数据无形中就增加了一个维度，变成了 [batch_size, sequence_length, embedding_size]\n",
    "\n",
    "之后使用 unsqueeze(1) 函数使数据增加一个维度，变成 [batch_size, 1, sequence_length, embedding_size]。现在的数据才能做卷积，因为在传统 CNN 中，输入数据就应该是 [batch_size, in_channel, height, width] 这种维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN,self).__init__()\n",
    "        self.W =nn.Embedding(vocab_size,embedding_size)\n",
    "        output_channel = 3\n",
    "        self.conv = nn.Sequential(\n",
    "            #conv [input_channel(=1), outputr_channel, (filter_height,filter_width),stride=1]\n",
    "            #这里的filter height可以自己定义\n",
    "            nn.Conv2d(1,output_channel,(2,embedding_size)),\n",
    "            nn.ReLU(),\n",
    "            # [pool]((filter_height,filter_width))\n",
    "            nn.MaxPool2d((2,1))\n",
    "        )\n",
    "        #fc \n",
    "        self.fc = nn.Linear(output_channel, num_classes)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        '''\n",
    "        X [batch_size, seq_len]\n",
    "        '''\n",
    "        batch_size = X.shape[0]\n",
    "        emb_X = self.W(X) # [batch_size, sequence_length, embedding_size]\n",
    "        #下一步在CV里用的比较多\n",
    "        emb_X = emb_X.unsqueeze(1) # add channel(=1) [batch, channel(=1), sequence_length, embedding_size]\n",
    "        conved = self .conv(emb_X)\n",
    "        flatten = conved.view(batch_size, -1)\n",
    "        output = self.fc(flatten)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 loss = 0.004269\n",
      "Epoch: 1000 loss = 0.001656\n",
      "Epoch: 2000 loss = 0.000403\n",
      "Epoch: 2000 loss = 0.000367\n",
      "Epoch: 3000 loss = 0.000088\n",
      "Epoch: 3000 loss = 0.000105\n",
      "Epoch: 4000 loss = 0.000025\n",
      "Epoch: 4000 loss = 0.000031\n",
      "Epoch: 5000 loss = 0.000010\n",
      "Epoch: 5000 loss = 0.000007\n",
      "Epoch: 6000 loss = 0.000003\n",
      "Epoch: 6000 loss = 0.000003\n",
      "Epoch: 7000 loss = 0.000001\n",
      "Epoch: 7000 loss = 0.000001\n",
      "Epoch: 8000 loss = 0.000000\n",
      "Epoch: 8000 loss = 0.000000\n",
      "Epoch: 9000 loss = 0.000000\n",
      "Epoch: 9000 loss = 0.000000\n",
      "Epoch: 10000 loss = 0.000000\n",
      "Epoch: 10000 loss = 0.000000\n",
      "Epoch: 11000 loss = 0.000000\n",
      "Epoch: 11000 loss = 0.000000\n",
      "Epoch: 12000 loss = 0.000000\n",
      "Epoch: 12000 loss = 0.000000\n",
      "Epoch: 13000 loss = 0.000000\n",
      "Epoch: 13000 loss = 0.000000\n",
      "Epoch: 14000 loss = 0.000000\n",
      "Epoch: 14000 loss = 0.000000\n",
      "Epoch: 15000 loss = 0.000000\n",
      "Epoch: 15000 loss = 0.000000\n",
      "Epoch: 16000 loss = 0.000000\n",
      "Epoch: 16000 loss = 0.000000\n",
      "Epoch: 17000 loss = 0.000000\n",
      "Epoch: 17000 loss = 0.000000\n",
      "Epoch: 18000 loss = 0.000000\n",
      "Epoch: 18000 loss = 0.000000\n",
      "Epoch: 19000 loss = 0.000000\n",
      "Epoch: 19000 loss = 0.000000\n",
      "Epoch: 20000 loss = 0.000000\n",
      "Epoch: 20000 loss = 0.000000\n",
      "Epoch: 21000 loss = 0.000000\n",
      "Epoch: 21000 loss = 0.000000\n",
      "Epoch: 22000 loss = 0.000000\n",
      "Epoch: 22000 loss = 0.000000\n",
      "Epoch: 23000 loss = 0.000000\n",
      "Epoch: 23000 loss = 0.000000\n",
      "Epoch: 24000 loss = 0.000000\n",
      "Epoch: 24000 loss = 0.000000\n",
      "Epoch: 25000 loss = 0.000000\n",
      "Epoch: 25000 loss = 0.000000\n",
      "Epoch: 26000 loss = 0.000000\n",
      "Epoch: 26000 loss = 0.000000\n",
      "Epoch: 27000 loss = 0.000000\n",
      "Epoch: 27000 loss = 0.000000\n",
      "Epoch: 28000 loss = 0.000000\n",
      "Epoch: 28000 loss = 0.000000\n",
      "Epoch: 29000 loss = 0.000000\n",
      "Epoch: 29000 loss = 0.000000\n",
      "Epoch: 30000 loss = 0.000000\n",
      "Epoch: 30000 loss = 0.000000\n",
      "Epoch: 31000 loss = 0.000000\n",
      "Epoch: 31000 loss = 0.000000\n",
      "Epoch: 32000 loss = 0.000000\n",
      "Epoch: 32000 loss = 0.000000\n",
      "Epoch: 33000 loss = 0.000000\n",
      "Epoch: 33000 loss = 0.000000\n",
      "Epoch: 34000 loss = 0.000000\n",
      "Epoch: 34000 loss = 0.000000\n",
      "Epoch: 35000 loss = 0.000000\n",
      "Epoch: 35000 loss = 0.000000\n",
      "Epoch: 36000 loss = 0.000000\n",
      "Epoch: 36000 loss = 0.000000\n",
      "Epoch: 37000 loss = 0.000000\n",
      "Epoch: 37000 loss = 0.000000\n",
      "Epoch: 38000 loss = 0.000000\n",
      "Epoch: 38000 loss = 0.000000\n",
      "Epoch: 39000 loss = 0.000000\n",
      "Epoch: 39000 loss = 0.000000\n",
      "Epoch: 40000 loss = 0.000000\n",
      "Epoch: 40000 loss = 0.000000\n",
      "Epoch: 41000 loss = 0.000000\n",
      "Epoch: 41000 loss = 0.000000\n",
      "Epoch: 42000 loss = 0.000000\n",
      "Epoch: 42000 loss = 0.000000\n",
      "Epoch: 43000 loss = 0.000000\n",
      "Epoch: 43000 loss = 0.000000\n",
      "Epoch: 44000 loss = 0.000000\n",
      "Epoch: 44000 loss = 0.000000\n",
      "Epoch: 45000 loss = 0.000000\n",
      "Epoch: 45000 loss = 0.000000\n",
      "Epoch: 46000 loss = 0.000000\n",
      "Epoch: 46000 loss = 0.000000\n",
      "Epoch: 47000 loss = 0.000000\n",
      "Epoch: 47000 loss = 0.000000\n",
      "Epoch: 48000 loss = 0.000000\n",
      "Epoch: 48000 loss = 0.000000\n",
      "Epoch: 49000 loss = 0.000000\n",
      "Epoch: 49000 loss = 0.000000\n",
      "Epoch: 50000 loss = 0.000000\n",
      "Epoch: 50000 loss = 0.000000\n"
     ]
    }
   ],
   "source": [
    "#Training \n",
    "for epoch in range(50000):\n",
    "    for batch_x,batch_y in loader:\n",
    "        pred = model(batch_x)\n",
    "        loss = criterion(pred,batch_y)\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i hate me is Bad Mean...\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_text = 'i hate me'\n",
    "tests = [[word2idx[n] for n in test_text.split()]]\n",
    "test_batch = torch.LongTensor(tests).to(device)\n",
    "# Predict\n",
    "model = model.eval()\n",
    "predict = model(test_batch).data.max(1, keepdim=True)[1]\n",
    "if predict[0][0] == 0:\n",
    "    print(test_text,\"is Bad Mean...\")\n",
    "else:\n",
    "    print(test_text,\"is Good Mean!!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "748dbdca4f5d9507dbf5438deb9fb0c5af4d959a1362599ffdf2eaf1f99424f2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
