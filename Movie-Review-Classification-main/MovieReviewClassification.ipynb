{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "411833_LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZq2syCQJ377",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed14d6b6-5dda-4675-ed3a-20ca03d5fd8e"
      },
      "source": [
        "#importing libraries\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYrFkI7sU65y"
      },
      "source": [
        "#For unzipping dataset from google drive\r\n",
        "# !unrar x \"/content/drive/MyDrive/review_polarity.rar\" \"/content/drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0gtTPhaVFua"
      },
      "source": [
        "#processing data on \"neg\" folder and cleaning the data with scikit learn inbuilt libraries\r\n",
        "\r\n",
        "import glob\r\n",
        "txt_files = glob.glob(\"/content/drive/MyDrive/review_polarity/txt_sentoken/neg/*.txt\")\r\n",
        "\r\n",
        "corpus = []\r\n",
        "corpus2 =[]\r\n",
        "booleans = []\r\n",
        "\r\n",
        "for i in range(len(txt_files)):\r\n",
        "  booleans.append(0)\r\n",
        "\r\n",
        "  with open(txt_files[i], 'r') as file:\r\n",
        "    data = file.read().replace('\\n', '')\r\n",
        "    review = re.sub('[^a-zA-Z]', ' ', data)\r\n",
        "    review = review.lower()\r\n",
        "    review = review.split()\r\n",
        "    ps = PorterStemmer()\r\n",
        "    all_stopwords = stopwords.words('english')\r\n",
        "    all_stopwords.remove('not')\r\n",
        "    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\r\n",
        "    review = ' '.join(review)\r\n",
        "    corpus.append(review)\r\n",
        "    helper =  nltk.WordPunctTokenizer().tokenize(review)\r\n",
        "    corpus2.append(helper)\r\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxB5fkdSbVyG"
      },
      "source": [
        "#processing data on \"pos\" folder and cleaning the data with scikit learn inbuilt libraries\r\n",
        "\r\n",
        "import glob\r\n",
        "txt_files = glob.glob(\"/content/drive/MyDrive/review_polarity/txt_sentoken/pos/*.txt\")\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "for i in range(len(txt_files)):\r\n",
        "  booleans.append(1)\r\n",
        "  with open(txt_files[i], 'r') as file:\r\n",
        "    data = file.read().replace('\\n', '')\r\n",
        "    review = re.sub('[^a-zA-Z]', ' ', data)\r\n",
        "    review = review.lower()\r\n",
        "    review = review.split()\r\n",
        "    ps = PorterStemmer()\r\n",
        "    all_stopwords = stopwords.words('english')\r\n",
        "    all_stopwords.remove('not')\r\n",
        "    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\r\n",
        "    review = ' '.join(review)\r\n",
        "    corpus.append(review)\r\n",
        "    helper =  nltk.WordPunctTokenizer().tokenize(review)\r\n",
        "    corpus2.append(helper)\r\n",
        "    # print(helper)\r\n",
        "   \r\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw5rp--duorV",
        "outputId": "162a2a90-c937-40ed-fc83-5e1e2ae6d2c1"
      },
      "source": [
        "print(len(booleans))\r\n",
        "print(len(corpus2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmoqS97cv-V-"
      },
      "source": [
        "#Tokeinizing words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDlQSSPv6Lsf",
        "outputId": "b15fb497-1b3e-48a2-8ab9-db3e5a686b43"
      },
      "source": [
        "#tokeninzing the words ( Building bag of words model )\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "cv = CountVectorizer(  max_features=1400)\r\n",
        "X = cv.fit_transform(corpus).toarray()\r\n",
        "\r\n",
        "print(len(X[0]))\r\n",
        "print(len(booleans))\r\n",
        "print(len(X))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1400\n",
            "2000\n",
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieEAaJ-d6OU6"
      },
      "source": [
        "# Generating test and train data set \r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, booleans, test_size = 0.20, random_state = 0)# Generating test and train data set \r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, booleans, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzPQX1W1sS0T"
      },
      "source": [
        "#Naive Bayes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah0ZNaJZhU-s",
        "outputId": "409b12d8-0c16-4ed7-b043-f54aa2deef8a"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "classifier = GaussianNB()\r\n",
        "\r\n",
        "classifier.fit(X_train, y_train)\r\n",
        "y_pred = classifier.predict(X_test)\r\n",
        "\r\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\r\n",
        "cm = confusion_matrix(y_test, y_pred)\r\n",
        "print(cm)\r\n",
        "print(\"accuracy of model :\" ,accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[156  44]\n",
            " [ 69 131]]\n",
            "accuracy of model : 0.7175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6i3AJ4QX81j",
        "outputId": "e8d8ef7c-b78c-49ae-ebcd-09144b54266c"
      },
      "source": [
        "for i in X_train[3]:\r\n",
        "  print(i , end =\" \")\r\n",
        "print(len(X_train[2]) ,end=\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 0 0 1 0 0 0 0 1 0 2 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 9 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 3 2 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 3 0 0 0 0 0 1 1 1 0 0 2 0 1 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 3 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 2 0 0 0 1 2 0 2 0 2 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 2 0 0 0 0 2 0 0 2 0 0 0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 1 0 2 0 0 1 0 0 0 1 0 0 2 0 0 0 4 0 0 0 0 1 0 0 0 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 5 0 0 0 0 0 0 0 0 0 1 0 0 0 0 2 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 2 0 1 0 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 3 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 1 4 0 0 0 0 0 0 2 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 1 0 0 0 2 0 0 0 0 0 1 0 2 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 3 0 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 2 1 0 0 0 0 0 1 0 0 1 0 1400 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX2UzrlYsY7P"
      },
      "source": [
        "#SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQQwU-EVj5TW",
        "outputId": "dff6b405-fa59-4813-d014-fa460a1cef95"
      },
      "source": [
        "from sklearn.svm import SVC\r\n",
        "classifier = SVC(kernel = 'rbf', random_state = 0)\r\n",
        "classifier.fit(X_train, y_train)\r\n",
        "print(len(X_train) , len(y_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1600 1600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJq0de31j-za",
        "outputId": "eec6d029-ae73-4caa-cfc0-f54a9c75b552"
      },
      "source": [
        "y_pred1 = classifier.predict(X_test)\r\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\r\n",
        "cm = confusion_matrix(y_test, y_pred1)\r\n",
        "print(cm)\r\n",
        "print(\"accuracy of model :\" , np.around(accuracy_score(y_test, y_pred1)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[161  39]\n",
            " [ 32 168]]\n",
            "accuracy of model : 82.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oSCrBd5FSY"
      },
      "source": [
        "#Max Entropy Classifer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5LvrIH_5Kqa",
        "outputId": "b1d18687-a958-453e-a9a2-5dc10e63e5bf"
      },
      "source": [
        "from sklearn import linear_model\r\n",
        "#(X, y) = (features matrix, labels)\r\n",
        "maxent = linear_model.LogisticRegression(penalty='l2', C=0.1)\r\n",
        "maxent.fit(X_train, y_train)\r\n",
        "\r\n",
        "y_predicted = maxent.predict(X_test)\r\n",
        "\r\n",
        "print(\"accuracy of model :\" ,np.around(accuracy_score(y_test, y_predicted)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy of model : 84.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIxGpP2ev6cl"
      },
      "source": [
        "#Implementing on bi-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU8kyxULwDny",
        "outputId": "7419ce61-11e1-4ce9-d716-7806c0907d81"
      },
      "source": [
        "#creating  a tokenizer model\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "bow_converter = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\r\n",
        "x = bow_converter.fit_transform(corpus2)\r\n",
        "\r\n",
        "words = bow_converter.get_feature_names()\r\n",
        "len(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25297"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OIojMXE2DoC"
      },
      "source": [
        "#Generating test and train data\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "training_data, test_data , y_train1 , y_test1 = train_test_split(corpus2 , booleans, train_size = 0.25, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk48t6Fo2F-1",
        "outputId": "276ce35e-5d43-4ffb-e548-b4f7a181c9dc"
      },
      "source": [
        "# building a bigram \r\n",
        "\r\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[2,2], lowercase=False , max_features=350000)\r\n",
        "\r\n",
        "X_tr_bow = bow_transform.fit_transform(training_data)\r\n",
        "X_te_bow = bow_transform.transform(test_data).toarray()\r\n",
        "\r\n",
        "bigrams = bow_transform.get_feature_names()\r\n",
        "\r\n",
        "X_tr_bow  = X_tr_bow.toarray()\r\n",
        "\r\n",
        "\r\n",
        "y_tr = y_train1\r\n",
        "y_te = y_test1\r\n",
        "print(bigrams[-10 :])\r\n",
        "\r\n",
        "# print(len(y_tr))\r\n",
        "# print(X_tr_bow[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['zuko sandi', 'zundel face', 'zundel speak', 'zwick co', 'zwick deserv', 'zwick screenwrit', 'zwick sieg', 'zwigoff brilliant', 'zyci master', 'zyci za']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P7SXHrO2I2A",
        "outputId": "87609611-6f23-442a-844a-580e8c50a662"
      },
      "source": [
        "#Gaussian model \r\n",
        "\r\n",
        "\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "classifier = GaussianNB( )\r\n",
        "\r\n",
        "classifier.fit(X_tr_bow, y_tr)\r\n",
        "y_pred = classifier.predict(X_te_bow)\r\n",
        "\r\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\r\n",
        "cm = confusion_matrix(y_te, y_pred)\r\n",
        "print(cm)\r\n",
        "print(\"accuracy :\" ,round(accuracy_score(y_te, y_pred)*100))\r\n",
        "\r\n",
        "\r\n",
        "#Bigram model gave a low accurac compared to"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[508 254]\n",
            " [203 535]]\n",
            "accuracy : 70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Jx5iHGEdKq1",
        "outputId": "f4495d38-ca60-450b-c2aa-2e20e5a5213d"
      },
      "source": [
        "print(len(X_train))\r\n",
        "print(len(y_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1600\n",
            "1600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llJ-E_yNjjGA"
      },
      "source": [
        "#Logisitic Regression from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LZdl1fMdT7R"
      },
      "source": [
        "#Sigmod function \r\n",
        "\r\n",
        "def sigmoid(input):    \r\n",
        "    output = 1 / (1 + np.exp(-input))\r\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9i_FRPVda-R",
        "outputId": "033ce745-fe83-4138-d6f7-671850975a3c"
      },
      "source": [
        "# Creating weights and bias variables\r\n",
        "\r\n",
        "init_parameters = {} \r\n",
        "init_parameters[\"weight\"] = np.zeros(X_train.shape[1])\r\n",
        "init_parameters[\"bias\"] = 0\r\n",
        "print(init_parameters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'weight': array([0., 0., 0., ..., 0., 0., 0.]), 'bias': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeC709EKdjVT"
      },
      "source": [
        "# Optimization function \r\n",
        "\r\n",
        "def optimize(X, Y,learning_rate,iterations,parameters): \r\n",
        "    size = X.shape[0]\r\n",
        "  \r\n",
        "    weight = parameters[\"weight\"] \r\n",
        "   \r\n",
        "    bias = parameters[\"bias\"]\r\n",
        "\r\n",
        "    for i in range(iterations): \r\n",
        "        sigma = sigmoid(np.dot(X, weight) + bias)\r\n",
        "      \r\n",
        "        loss = -1/size * np.sum(Y * np.log(sigma)) + (np.ones(len(Y)) - Y) * np.log(1-sigma)\r\n",
        "\r\n",
        "      \r\n",
        "        dW = 1/size * np.dot(X.T, (sigma - Y))\r\n",
        "        db = 1/size * np.sum(sigma - Y)\r\n",
        "        weight -= learning_rate * dW\r\n",
        "        bias -= learning_rate * db \r\n",
        "    \r\n",
        "    parameters[\"weight\"] = weight\r\n",
        "    parameters[\"bias\"] = bias\r\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHBSTRhodsIK"
      },
      "source": [
        "# Building model\r\n",
        "\r\n",
        "def train(X, Y, learning_rate,iterations):\r\n",
        "    print(len(X))\r\n",
        "    print(len(Y))\r\n",
        "    parameters_out = optimize(X, Y, learning_rate, iterations ,init_parameters)\r\n",
        "    return parameters_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldKOYA5-eGC4",
        "outputId": "4caea3b7-f618-4576-ad3d-fe57dc5afd24"
      },
      "source": [
        "\r\n",
        "parameters_out = train(X_train, y_train, learning_rate = 0.02, iterations = 500)\r\n",
        "parameters_out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1600\n",
            "1600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bias': -0.1362041367079207,\n",
              " 'weight': array([-0.07649132,  0.10273034,  0.02794866, ...,  0.00074313,\n",
              "        -0.00358161, -0.00405264])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnU0QN_ZiC2C",
        "outputId": "5158b82e-3a58-4eea-9e87-0ba3129e01b6"
      },
      "source": [
        "\r\n",
        "#predicition of test set data values\r\n",
        "\r\n",
        "output_values = np.dot(X_test, parameters_out[\"weight\"]) + parameters_out[\"bias\"]\r\n",
        "\r\n",
        "\r\n",
        "predictions = sigmoid(output_values) >= 1/2\r\n",
        "predictions\r\n",
        "print(\"Accuracy\" , np.around(accuracy_score(y_test , predictions)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 84.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}