我们希望把单词转换成一个vector

![1608124945265](E:\笔记\机器学习\词向量\1608124945265.png)

人们一开始发明了onehot来表示单词

![1608125011103](E:\笔记\机器学习\词向量\1608125011103.png)

这里存在一个问题 就是onehot每发表示单词的语义

例如 漂亮和美丽没法同时表达



![1608125090973](E:\笔记\机器学习\词向量\1608125090973.png)

tfidf 常见的单词给他一个高一点的权重 不常见的单词给他一个低一点的权重

![1608125175221](E:\笔记\机器学习\词向量\1608125175221.png)

考虑了词的顺序 但是会导致词表膨胀

离散表示存在的问题：

![1608125234450](E:\笔记\机器学习\词向量\1608125234450.png)

![1608126913175](E:\笔记\机器学习\词向量\1608126913175.png)

空间向量上要能涵盖他的意思

![1608126935401](E:\笔记\机器学习\词向量\1608126935401.png)

假如我们还要希望西班牙语和英语的向量空间很相似

![1608175681535](E:\笔记\机器学习\词向量\1608175681535.png)

![1608176456461](E:\笔记\机器学习\词向量\1608176456461.png)

根据中心词 预测周围词

输入一个中心词 wt 用一个一层神经网络预测他周围的单词

我们的目标是训练一个非常好的词向量

uo 和 uc的乘机表示他俩的相似度  乘机越大 表示他俩的相似度越高

uw和uc相乘 会让计算变得很慢 

如何解决呢 就是取对数

![1608177815500](E:\笔记\机器学习\词向量\1608177815500.png)

现在假设我们有50000个单词，每个单词有100维 

input embedding：50000*100

进来一个单词我们就把这一行取出来

相应的我们也有一个output embedding

output embedding： 50000*100

uo呢就是我们从output里面拿出单词的那个行出来

vc呢就是我们从intput里面拿出单词的那个行出来

这里有个疑惑就是到底是onehot作为输入呢 还是随机100维输入呢

有人说：

onehot乘在embedding上不久成了一个100维的了。embedding矩阵随机初始化的。

如果用onehot呢 就会导致输入的单词表太大的问题？

![1608178391890](E:\笔记\机器学习\词向量\1608178391890.png)

假设我们现在做一个二分类的任务

给我一个中心词 给我一个周围词

看看这个周围词是否是中心词的周围词 如果是预测为1 不是就为0

负例采样怎么做呢：

你首先给我一个vc中心词 和 uw 正确的周围单词 还要若干个错误的周围词，这些周围 词就从单词表中随机生成

样本集由正样本（正常的一句话）和负样本（可以将正常的话反着写）组成

强推cs224n nlp这个就是第一课的内容，超级容易懂

