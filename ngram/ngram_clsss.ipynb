{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import pandas as pd\r\n",
    "import pickle\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.linear_model import LogisticRegression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "#读取数据\r\n",
    "file_path={'sst2':'SST2/train.tsv'}\r\n",
    "df_list = []\r\n",
    "for source, file in file_path.items():\r\n",
    "     df=pd.read_csv(file,names=['sentence','label'],sep='\\t')\r\n",
    "     df_list.append(df)\r\n",
    "df = pd.concat(df_list)\r\n",
    "print(df.head())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                            sentence  label\n",
      "0  a stirring , funny and finally transporting re...      1\n",
      "1  apparently reassembled from the cutting room f...      0\n",
      "2  they presume their audience wo n't sit still f...      0\n",
      "3  this is a visually stunning rumination on love...      1\n",
      "4  jonathan parker 's bartleby should have been t...      1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "print(type(df))\r\n",
    "print(df['sentence'])\r\n",
    "#print(dl['splitset_label'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "0       a stirring , funny and finally transporting re...\n",
      "1       apparently reassembled from the cutting room f...\n",
      "2       they presume their audience wo n't sit still f...\n",
      "3       this is a visually stunning rumination on love...\n",
      "4       jonathan parker 's bartleby should have been t...\n",
      "                              ...                        \n",
      "6915    painful , horrifying and oppressively tragic ,...\n",
      "6916    take care is nicely performed by a quintet of ...\n",
      "6917    the script covers huge , heavy topics in a bla...\n",
      "6918    a seriously bad film with seriously warped log...\n",
      "6919    a deliciously nonsensical comedy about a city ...\n",
      "Name: sentence, Length: 6920, dtype: object\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import re\r\n",
    "import string\r\n",
    "from collections import OrderedDict\r\n",
    "\r\n",
    "def cleanInput(input):\r\n",
    "    input = re.sub('\\n+', \" \", input)\r\n",
    "    input = re.sub('\\[[0-9]*\\]', \"\", input)\r\n",
    "    input = re.sub(' +', \" \", input)\r\n",
    "    input = bytes(input, \"UTF-8\")\r\n",
    "    input = input.decode(\"ascii\", \"ignore\")\r\n",
    "    cleanInput = []\r\n",
    "    input = input.split(' ')\r\n",
    "    for item in input:\r\n",
    "        item = item.strip(string.punctuation)\r\n",
    "        if len(item) > 1 or (item.lower() == 'a' or item.lower() == 'i'):\r\n",
    "            cleanInput.append(item)\r\n",
    "    return cleanInput\r\n",
    "\r\n",
    "def getNgrams(input, n):\r\n",
    "    input = cleanInput(input)\r\n",
    "    #print(input)\r\n",
    "    output = dict()\r\n",
    "    lis = []\r\n",
    "    for j in range(n+1):\r\n",
    "        if j ==0:\r\n",
    "            continue\r\n",
    "        for i in range(len(input)-j+1):\r\n",
    "            newNGram = \" \".join(input[i:i+j])\r\n",
    "            lis.append(newNGram)\r\n",
    "            if newNGram in output:\r\n",
    "                output[newNGram] += 1\r\n",
    "            else:\r\n",
    "                output[newNGram] = 1\r\n",
    "    return output,lis\r\n",
    "\r\n",
    "\r\n",
    "    # for i in range(len(input)-n+1):\r\n",
    "    #     newNGram = \" \".join(input[i:i+n])\r\n",
    "    #     lis.append(newNGram)\r\n",
    "    #     if newNGram in output:\r\n",
    "    #         output[newNGram] += 1\r\n",
    "    #     else:\r\n",
    "    #         output[newNGram] = 1\r\n",
    "    # return output,lis"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "output,lis = getNgrams(\"I'd like swimming i\",2)\r\n",
    "print(type(output))\r\n",
    "print(lis)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'dict'>\n",
      "[\"I'd\", 'like', 'swimming', 'i', \"I'd like\", 'like swimming', 'swimming i']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# coding:utf-8\r\n",
    "import numpy as np\r\n",
    "from scipy import sparse\r\n",
    "\r\n",
    "class MyCountVectorizer():\r\n",
    "    def __init__(self, ngram_range=1, pass_stop=True):\r\n",
    "        self.pass_stop = pass_stop # 提供停止词滤除功能，可禁止\r\n",
    "        self.ngram_range = ngram_range #提供ngram的功能\r\n",
    "        \r\n",
    "    def fit(self, data):\r\n",
    "        n_data = data #使用ngram的部分数据\r\n",
    "        # data = map(lambda x:str(x).split(\" \"), data)\r\n",
    "        self.elements_ = set()\r\n",
    "        # if self.ngram_range ==1:\r\n",
    "        #     for line in data:\r\n",
    "        #         for x in line:\r\n",
    "        #             if self.pass_stop:\r\n",
    "        #                 if len(x)==1:\r\n",
    "        #                     continue\r\n",
    "        #             self.elements_.add(x)\r\n",
    "        # else:\r\n",
    "        for line in n_data:\r\n",
    "            dic,li = getNgrams(line,self.ngram_range)\r\n",
    "            for x in li:\r\n",
    "                # if self.pass_stop:\r\n",
    "                #     if len(x)==1:\r\n",
    "                #         continue\r\n",
    "                self.elements_.add(x)\r\n",
    "        # 原元素\r\n",
    "        self.elements_ = np.sort(list(self.elements_))\r\n",
    "        # 编码\r\n",
    "        self.labels_ = np.arange(len(self.elements_)).astype(int)\r\n",
    "        # 生成字典\r\n",
    "        self.dict_ = {}\r\n",
    "        for i in range(len(self.elements_)):\r\n",
    "            self.dict_[str(self.elements_[i])] = self.labels_[i]\r\n",
    "\r\n",
    "    def transform(self, data):\r\n",
    "        rows = []\r\n",
    "        cols = []\r\n",
    "        n_data = data\r\n",
    "        data = map(lambda x:str(x).split(\" \"), data)\r\n",
    "        l_data = list(data)\r\n",
    "        #print(data)\r\n",
    "        #print(n_data.size)\r\n",
    "\r\n",
    "        # for i in range(len(l_data)):\r\n",
    "        #     for x in l_data[i]:\r\n",
    "        #         if self.pass_stop:\r\n",
    "        #             if len(x)==1:\r\n",
    "        #                 continue\r\n",
    "        #         rows.append(i)\r\n",
    "        #         print(\"这一次的输出i是%d,这一次输出的x是%d\",i,x)\r\n",
    "        #         cols.append(self.dict_[x])\r\n",
    "        index =0;\r\n",
    "        count = 0;\r\n",
    "        cn =0;\r\n",
    "        for line in n_data:\r\n",
    "            dic,li = getNgrams(line,self.ngram_range)\r\n",
    "            for x in li:\r\n",
    "                if self.pass_stop:\r\n",
    "                    if len(x)==1:\r\n",
    "                        continue\r\n",
    "               #print(\"这一次的输出i是%d,这一次输出的x是%d\",index,x)\r\n",
    "                \r\n",
    "                if x in self.dict_:\r\n",
    "                    rows.append(index)\r\n",
    "                    cols.append(self.dict_[x])\r\n",
    "                    cn = cn+1\r\n",
    "                else:\r\n",
    "                    count = count+1\r\n",
    "                    # self.dict_[x]\r\n",
    "                    # cols.append(self.dict_[x])\r\n",
    "                    #print(x)\r\n",
    "            \r\n",
    "            index = index + 1 \r\n",
    "\r\n",
    "\r\n",
    "        vals = np.ones((len(rows),)).astype(int)\r\n",
    "        print(\"cn是\",cn)\r\n",
    "        print(\"count是\",count)\r\n",
    "        #return sparse.csr_matrix((vals, (rows, cols)), shape=(len(l_data), len(self.labels_)))\r\n",
    "        return sparse.csr_matrix((vals, (rows, cols)), shape=(index, len(self.labels_)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "sentence = df['sentence'].values[:1000]\r\n",
    "#print(sentence[:100])\r\n",
    "#print(type(sentence))\r\n",
    "y = df['label'].values[:1000]\r\n",
    "#划分处数据集\r\n",
    "sentence_train,sentence_test,y_train,y_test=train_test_split(sentence,y,test_size=0.25,random_state=1000)\r\n",
    "#print(type(y_train))#<class 'numpy.ndarray'>\r\n",
    "vectorize = MyCountVectorizer(ngram_range=2)\r\n",
    "vectorize.fit(sentence)\r\n",
    "X_train = vectorize.transform(sentence_train)\r\n",
    "X_test = vectorize.transform(sentence_test)\r\n",
    "print(X_train.shape)\r\n",
    "#print(X_train.shape)#(5190, 203644)\r\n",
    "#print(type(X_train))#'scipy.sparse.csr.csr_matrix'\r\n",
    "xtra = X_train.toarray()\r\n",
    "xtes = X_test.toarray()\r\n",
    "#print(type(xt))#<class 'numpy.ndarray'>\r\n",
    "#源代码\r\n",
    "# sentence = df['sentence'].values[:1000]\r\n",
    "# #print(sentence[:100])\r\n",
    "# #print(type(sentence))\r\n",
    "# y = df['label'].values[:1000]\r\n",
    "# #划分处数据集\r\n",
    "# sentence_train,sentence_test,y_train,y_test=train_test_split(sentence,y,test_size=0.25,random_state=1000)\r\n",
    "# #print(type(y_train))#<class 'numpy.ndarray'>\r\n",
    "# vectorize = CountVectorizer(min_df=0,lowercase=False,ngram_range=(1,4))\r\n",
    "# vectorize.fit(sentence_train)\r\n",
    "# X_train = vectorize.transform(sentence_train)\r\n",
    "# X_test = vectorize.transform(sentence_test)\r\n",
    "# #print(X_train.shape)#(5190, 203644)\r\n",
    "# #print(type(X_train))#'scipy.sparse.csr.csr_matrix'\r\n",
    "# #xt = X_train.toarray()\r\n",
    "# #print(type(xt))#<class 'numpy.ndarray'>"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cn是 24691\n",
      "count是 0\n",
      "cn是 8419\n",
      "count是 0\n",
      "(750, 17526)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "#查看更换后的数据\r\n",
    "#print(xt.shape[0])\r\n",
    "# s = set()\r\n",
    "# for i in range(xt.shape[0]):\r\n",
    "#     for j in range(xt.shape[1]):\r\n",
    "#         if xt[i][j] > 1:\r\n",
    "#             #print(xt[i][j])\r\n",
    "#             s.add(xt[i][j])\r\n",
    "# print(s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "classifier = LogisticRegression()\r\n",
    "classifier.fit(X_train,y_train)\r\n",
    "print(type(classifier))\r\n",
    "score = classifier.score(X_test,y_test)\r\n",
    "print(score)#自己的是076"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "0.676\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "import time\r\n",
    "import numpy as np\r\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "class maxEnt:\r\n",
    "    '''\r\n",
    "    最大熵类\r\n",
    "    '''\r\n",
    "    def __init__(self, trainDataList, trainLabelList, testDataList, testLabelList):\r\n",
    "        '''\r\n",
    "        各参数初始化\r\n",
    "        '''\r\n",
    "        self.trainDataList = trainDataList          #训练数据集\r\n",
    "        self.trainLabelList = trainLabelList        #训练标签集\r\n",
    "        self.testDataList = testDataList            #测试数据集\r\n",
    "        self.testLabelList = testLabelList          #测试标签集\r\n",
    "        self.featureNum = len(trainDataList[0])     #特征数量\r\n",
    "        self.N = len(trainDataList)                 #总训练集长度\r\n",
    "        self.n = 0                                  #训练集中（xi，y）对数量\r\n",
    "        self.M = 10000                                 #\r\n",
    "        self.fixy = self.calc_fixy()                #所有(x, y)对出现的次数\r\n",
    "        self.w = [0] * self.n                       #Pw(y|x)中的w\r\n",
    "        self.xy2idDict, self.id2xyDict = self.createSearchDict()        #(x, y)->id和id->(x, y)的搜索字典\r\n",
    "        self.Ep_xy = self.calcEp_xy()               #Ep_xy期望值\r\n",
    "    def calcEpxy(self):\r\n",
    "        '''\r\n",
    "        计算特征函数f(x, y)关于模型P(Y|X)与经验分布P_(X, Y)的期望值（P后带下划线“_”表示P上方的横线\r\n",
    "        程序中部分下划线表示“|”，部分表示上方横线，请根据具体公式自行判断,）\r\n",
    "        即“6.2.2 最大熵模型的定义”中第二个期望（83页最上方的期望）\r\n",
    "        :return:\r\n",
    "        '''\r\n",
    "        #初始化期望存放列表，对于每一个xy对都有一个期望\r\n",
    "        #这里的x是单个的特征，不是一个样本的全部特征。例如x={x1，x2，x3.....，xk}，实际上是（x1，y），（x2，y），。。。\r\n",
    "        #但是在存放过程中需要将不同特诊的分开存放，李航的书可能是为了公式的泛化性高一点，所以没有对这部分提及\r\n",
    "        #具体可以看我的博客，里面有详细介绍  www.pkudodo.com\r\n",
    "        Epxy = [0] * self.n\r\n",
    "        #对于每一个样本进行遍历\r\n",
    "        for i in range(self.N):\r\n",
    "            #初始化公式中的P(y|x)列表\r\n",
    "            Pwxy = [0] * 2\r\n",
    "            #计算P(y = 0 } X)\r\n",
    "            #注：程序中X表示是一个样本的全部特征，x表示单个特征，这里是全部特征的一个样本\r\n",
    "            Pwxy[0] = self.calcPwy_x(self.trainDataList[i], 0)\r\n",
    "            #计算P(y = 1 } X)\r\n",
    "            Pwxy[1] = self.calcPwy_x(self.trainDataList[i], 1)\r\n",
    "            for feature in range(self.featureNum):\r\n",
    "                for y in range(2):\r\n",
    "                    if (self.trainDataList[i][feature], y) in self.fixy[feature]:\r\n",
    "                        id = self.xy2idDict[feature][(self.trainDataList[i][feature], y)]\r\n",
    "                        Epxy[id] += (1 / self.N) * Pwxy[y]\r\n",
    "        return Epxy\r\n",
    "    def calcEp_xy(self):\r\n",
    "        '''\r\n",
    "        计算特征函数f(x, y)关于经验分布P_(x, y)的期望值（下划线表示P上方的横线，\r\n",
    "        同理Ep_xy中的“_”也表示p上方的横线）\r\n",
    "        即“6.2.2 最大熵的定义”中第一个期望（82页最下方那个式子）\r\n",
    "        :return: 计算得到的Ep_xy\r\n",
    "        '''\r\n",
    "        #初始化Ep_xy列表，长度为n\r\n",
    "        Ep_xy = [0] * self.n\r\n",
    "        #遍历每一个特征\r\n",
    "        for feature in range(self.featureNum):\r\n",
    "            #遍历每个特征中的(x, y)对\r\n",
    "            for (x, y) in self.fixy[feature]:\r\n",
    "                #获得其id\r\n",
    "                id = self.xy2idDict[feature][(x, y)]\r\n",
    "                #将计算得到的Ep_xy写入对应的位置中\r\n",
    "                #fixy中存放所有对在训练集中出现过的次数，处于训练集总长度N就是概率了\r\n",
    "                Ep_xy[id] = self.fixy[feature][(x, y)] / self.N\r\n",
    "        #返回期望\r\n",
    "        return Ep_xy\r\n",
    "    def createSearchDict(self):\r\n",
    "        '''\r\n",
    "        创建查询字典\r\n",
    "        xy2idDict：通过(x,y)对找到其id,所有出现过的xy对都有一个id\r\n",
    "        id2xyDict：通过id找到对应的(x,y)对\r\n",
    "        '''\r\n",
    "        #设置xy搜多id字典\r\n",
    "        #这里的x指的是单个的特征，而不是某个样本，因此将特征存入字典时也需要存入这是第几个特征\r\n",
    "        #这一信息，这是为了后续的方便，否则会乱套。\r\n",
    "        #比如说一个样本X = (0, 1, 1) label =(1)\r\n",
    "        #生成的标签对有(0, 1), (1, 1), (1, 1)，三个(x，y)对并不能判断属于哪个特征的，后续就没法往下写\r\n",
    "        #不可能通过(1, 1)就能找到对应的id，因为对于(1, 1),字典中有多重映射\r\n",
    "        #所以在生成字典的时总共生成了特征数个字典，例如在mnist中样本有784维特征，所以生成784个字典，属于\r\n",
    "        #不同特征的xy存入不同特征内的字典中，使其不会混淆\r\n",
    "        xy2idDict = [{} for i in range(self.featureNum)]\r\n",
    "        #初始化id到xy对的字典。因为id与(x，y)的指向是唯一的，所以可以使用一个字典\r\n",
    "        id2xyDict = {}\r\n",
    "        #设置缩影，其实就是最后的id\r\n",
    "        index = 0\r\n",
    "        #对特征进行遍历\r\n",
    "        for feature in range(self.featureNum):\r\n",
    "            #对出现过的每一个(x, y)对进行遍历\r\n",
    "            #fixy：内部存放特征数目个字典，对于遍历的每一个特征，单独读取对应字典内的(x, y)对\r\n",
    "            for (x, y) in self.fixy[feature]:\r\n",
    "                #将该(x, y)对存入字典中，要注意存入时通过[feature]指定了存入哪个特征内部的字典\r\n",
    "                #同时将index作为该对的id号\r\n",
    "                xy2idDict[feature][(x, y)] = index\r\n",
    "                #同时在id->xy字典中写入id号，val为(x, y)对\r\n",
    "                id2xyDict[index] = (x, y)\r\n",
    "                #id加一\r\n",
    "                index += 1\r\n",
    "        #返回创建的两个字典\r\n",
    "        return xy2idDict, id2xyDict\r\n",
    "    def calc_fixy(self):\r\n",
    "        '''\r\n",
    "        计算(x, y)在训练集中出现过的次数\r\n",
    "        :return:\r\n",
    "        '''\r\n",
    "        #建立特征数目个字典，属于不同特征的(x, y)对存入不同的字典中，保证不被混淆\r\n",
    "        fixyDict = [defaultdict(int) for i in range(self.featureNum)]\r\n",
    "        #遍历训练集中所有样本\r\n",
    "        for i in range(len(self.trainDataList)):\r\n",
    "            #遍历样本中所有特征\r\n",
    "            for j in range(self.featureNum):\r\n",
    "                #将出现过的(x, y)对放入字典中并计数值加1\r\n",
    "                fixyDict[j][(self.trainDataList[i][j], self.trainLabelList[i])] += 1\r\n",
    "        #对整个大字典进行计数，判断去重后还有多少(x, y)对，写入n\r\n",
    "        for i in fixyDict:\r\n",
    "            self.n += len(i)\r\n",
    "        #返回大字典\r\n",
    "        return fixyDict\r\n",
    "    def calcPwy_x(self, X, y):\r\n",
    "        '''\r\n",
    "        计算“6.23 最大熵模型的学习” 式6.22\r\n",
    "        :param X: 要计算的样本X（一个包含全部特征的样本）\r\n",
    "        :param y: 该样本的标签\r\n",
    "        :return: 计算得到的Pw(Y|X)\r\n",
    "        '''\r\n",
    "        #分子\r\n",
    "        numerator = 0\r\n",
    "        #分母\r\n",
    "        Z = 0\r\n",
    "        #对每个特征进行遍历\r\n",
    "        for i in range(self.featureNum):\r\n",
    "            #如果该(xi,y)对在训练集中出现过\r\n",
    "            if (X[i], y) in self.xy2idDict[i]:\r\n",
    "                #在xy->id字典中指定当前特征i，以及(x, y)对：(X[i], y)，读取其id\r\n",
    "                index = self.xy2idDict[i][(X[i], y)]\r\n",
    "                #分子是wi和fi(x，y)的连乘再求和，最后指数\r\n",
    "                #由于当(x, y)存在时fi(x，y)为1，因为xy对肯定存在，所以直接就是1\r\n",
    "                #对于分子来说，就是n个wi累加，最后再指数就可以了\r\n",
    "                #因为有n个w，所以通过id将w与xy绑定，前文的两个搜索字典中的id就是用在这里\r\n",
    "                numerator += self.w[index]\r\n",
    "            #同时计算其他一种标签y时候的分子，下面的z并不是全部的分母，再加上上式的分子以后\r\n",
    "            #才是完整的分母，即z = z + numerator\r\n",
    "            if (X[i], 1-y) in self.xy2idDict[i]:\r\n",
    "                #原理与上式相同\r\n",
    "                index = self.xy2idDict[i][(X[i], 1-y)]\r\n",
    "                Z += self.w[index]\r\n",
    "        #计算分子的指数\r\n",
    "        numerator = np.exp(numerator)\r\n",
    "        #计算分母的z\r\n",
    "        Z = np.exp(Z) + numerator\r\n",
    "        #返回Pw(y|x)\r\n",
    "        return numerator / Z\r\n",
    "    def maxEntropyTrain(self, iter = 144):#300\r\n",
    "        #设置迭代次数寻找最优解\r\n",
    "        for i in range(iter):\r\n",
    "            #单次迭代起始时间点\r\n",
    "            iterStart = time.time()\r\n",
    "            #计算“6.2.3 最大熵模型的学习”中的第二个期望（83页最上方哪个）\r\n",
    "            Epxy = self.calcEpxy()\r\n",
    "            #使用的是IIS，所以设置sigma列表\r\n",
    "            sigmaList = [0] * self.n\r\n",
    "            #对于所有的n进行一次遍历\r\n",
    "            for j in range(self.n):\r\n",
    "                #依据“6.3.1 改进的迭代尺度法” 式6.34计算\r\n",
    "                sigmaList[j] = (1 / self.M) * np.log(self.Ep_xy[j] / Epxy[j])\r\n",
    "            #按照算法6.1步骤二中的（b）更新w\r\n",
    "            self.w = [self.w[i] + sigmaList[i] for i in range(self.n)]\r\n",
    "            #单次迭代结束\r\n",
    "            iterEnd = time.time()\r\n",
    "            #打印运行时长信息\r\n",
    "            print('iter:%d:%d, time:%d'%(i, iter, iterEnd - iterStart))\r\n",
    "    def predict(self, X):\r\n",
    "        '''\r\n",
    "        预测标签\r\n",
    "        :param X:要预测的样本\r\n",
    "        :return: 预测值\r\n",
    "        '''\r\n",
    "        #因为y只有0和1，所有建立两个长度的概率列表\r\n",
    "        result = [0] * 2\r\n",
    "        #循环计算两个概率\r\n",
    "        for i in range(2):\r\n",
    "            #计算样本x的标签为i的概率\r\n",
    "            result[i] = self.calcPwy_x(X, i)\r\n",
    "        #返回标签\r\n",
    "        #max(result)：找到result中最大的那个概率值\r\n",
    "        #result.index(max(result))：通过最大的那个概率值再找到其索引，索引是0就返回0，1就返回1\r\n",
    "        return result.index(max(result))\r\n",
    "    def test(self):\r\n",
    "        '''\r\n",
    "        对测试集进行测试\r\n",
    "        :return:\r\n",
    "        '''\r\n",
    "        #错误值计数\r\n",
    "        errorCnt = 0\r\n",
    "        #对测试集中所有样本进行遍历\r\n",
    "        for i in range(len(self.testDataList)):\r\n",
    "            #预测该样本对应的标签\r\n",
    "            result = self.predict(self.testDataList[i])\r\n",
    "            #如果错误，计数值加1\r\n",
    "            if result != self.testLabelList[i]:   errorCnt += 1\r\n",
    "        #返回准确率\r\n",
    "        return 1 - errorCnt / len(self.testDataList)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "start = time.time()\r\n",
    "#先不二值化 看看效果如何\r\n",
    "maxEnt = maxEnt(xtra, y_train, xtes, y_test)\r\n",
    "#maxEnt = maxEnt(xtra[:100], y_train[:100], xtes[:100], y_test[:100])\r\n",
    "#开始训练\r\n",
    "print('start to train')\r\n",
    "maxEnt.maxEntropyTrain()\r\n",
    "#开始测试\r\n",
    "print('start to test')\r\n",
    "accuracy = maxEnt.test()\r\n",
    "print('the accuracy is:', accuracy)\r\n",
    "# 打印时间\r\n",
    "print('time span:', time.time() - start)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start to train\n",
      "iter:0:144, time:105\n",
      "iter:1:144, time:103\n",
      "iter:2:144, time:107\n",
      "iter:3:144, time:106\n",
      "iter:4:144, time:106\n",
      "iter:5:144, time:104\n",
      "iter:6:144, time:106\n",
      "iter:7:144, time:107\n",
      "iter:8:144, time:105\n",
      "iter:9:144, time:107\n",
      "iter:10:144, time:107\n",
      "iter:11:144, time:105\n",
      "iter:12:144, time:106\n",
      "iter:13:144, time:107\n",
      "iter:14:144, time:105\n",
      "iter:15:144, time:106\n",
      "iter:16:144, time:106\n",
      "iter:17:144, time:106\n",
      "iter:18:144, time:105\n",
      "iter:19:144, time:105\n",
      "iter:20:144, time:104\n",
      "iter:21:144, time:103\n",
      "iter:22:144, time:106\n",
      "iter:23:144, time:106\n",
      "iter:24:144, time:107\n",
      "iter:25:144, time:108\n",
      "iter:26:144, time:109\n",
      "iter:27:144, time:108\n",
      "iter:28:144, time:109\n",
      "iter:29:144, time:109\n",
      "iter:30:144, time:109\n",
      "iter:31:144, time:108\n",
      "iter:32:144, time:108\n",
      "iter:33:144, time:107\n",
      "iter:34:144, time:107\n",
      "iter:35:144, time:107\n",
      "iter:36:144, time:110\n",
      "iter:37:144, time:107\n",
      "iter:38:144, time:107\n",
      "iter:39:144, time:109\n",
      "iter:40:144, time:107\n",
      "iter:41:144, time:109\n",
      "iter:42:144, time:109\n",
      "iter:43:144, time:107\n",
      "iter:44:144, time:107\n",
      "iter:45:144, time:105\n",
      "iter:46:144, time:105\n",
      "iter:47:144, time:107\n",
      "iter:48:144, time:106\n",
      "iter:49:144, time:107\n",
      "iter:50:144, time:107\n",
      "iter:51:144, time:105\n",
      "iter:52:144, time:106\n",
      "iter:53:144, time:108\n",
      "iter:54:144, time:108\n",
      "iter:55:144, time:116\n",
      "iter:56:144, time:116\n",
      "iter:57:144, time:112\n",
      "iter:58:144, time:111\n",
      "iter:59:144, time:112\n",
      "iter:60:144, time:105\n",
      "iter:61:144, time:103\n",
      "iter:62:144, time:105\n",
      "iter:63:144, time:107\n",
      "iter:64:144, time:104\n",
      "iter:65:144, time:104\n",
      "iter:66:144, time:104\n",
      "iter:67:144, time:104\n",
      "iter:68:144, time:106\n",
      "iter:69:144, time:107\n",
      "iter:70:144, time:108\n",
      "iter:71:144, time:106\n",
      "iter:72:144, time:107\n",
      "iter:73:144, time:107\n",
      "iter:74:144, time:106\n",
      "iter:75:144, time:107\n",
      "iter:76:144, time:106\n",
      "iter:77:144, time:109\n",
      "iter:78:144, time:104\n",
      "iter:79:144, time:107\n",
      "iter:80:144, time:107\n",
      "iter:81:144, time:105\n",
      "iter:82:144, time:105\n",
      "iter:83:144, time:106\n",
      "iter:84:144, time:110\n",
      "iter:85:144, time:110\n",
      "iter:86:144, time:108\n",
      "iter:87:144, time:108\n",
      "iter:88:144, time:108\n",
      "iter:89:144, time:106\n",
      "iter:90:144, time:106\n",
      "iter:91:144, time:103\n",
      "iter:92:144, time:107\n",
      "iter:93:144, time:105\n",
      "iter:94:144, time:105\n",
      "iter:95:144, time:105\n",
      "iter:96:144, time:105\n",
      "iter:97:144, time:105\n",
      "iter:98:144, time:106\n",
      "iter:99:144, time:107\n",
      "iter:100:144, time:105\n",
      "iter:101:144, time:104\n",
      "iter:102:144, time:105\n",
      "iter:103:144, time:105\n",
      "iter:104:144, time:105\n",
      "iter:105:144, time:112\n",
      "iter:106:144, time:113\n",
      "iter:107:144, time:113\n",
      "iter:108:144, time:3198\n",
      "iter:109:144, time:114\n",
      "iter:110:144, time:111\n",
      "iter:111:144, time:113\n",
      "iter:112:144, time:114\n",
      "iter:113:144, time:114\n",
      "iter:114:144, time:114\n",
      "iter:115:144, time:116\n",
      "iter:116:144, time:110\n",
      "iter:117:144, time:105\n",
      "iter:118:144, time:106\n",
      "iter:119:144, time:105\n",
      "iter:120:144, time:108\n",
      "iter:121:144, time:107\n",
      "iter:122:144, time:106\n",
      "iter:123:144, time:105\n",
      "iter:124:144, time:105\n",
      "iter:125:144, time:107\n",
      "iter:126:144, time:106\n",
      "iter:127:144, time:106\n",
      "iter:128:144, time:107\n",
      "iter:129:144, time:107\n",
      "iter:130:144, time:107\n",
      "iter:131:144, time:107\n",
      "iter:132:144, time:105\n",
      "iter:133:144, time:106\n",
      "iter:134:144, time:105\n",
      "iter:135:144, time:105\n",
      "iter:136:144, time:106\n",
      "iter:137:144, time:106\n",
      "iter:138:144, time:106\n",
      "iter:139:144, time:106\n",
      "iter:140:144, time:105\n",
      "iter:141:144, time:105\n",
      "iter:142:144, time:106\n",
      "iter:143:144, time:106\n",
      "start to test\n",
      "the accuracy is: 0.56\n",
      "time span: 18605.644790649414\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "res = maxEnt.predict(xtes[0])\r\n",
    "print(res)\r\n",
    "print(y_test[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "748dbdca4f5d9507dbf5438deb9fb0c5af4d959a1362599ffdf2eaf1f99424f2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}