{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from random import *\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert主要难在前面的数据处理上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    'Hello, how are you? I am Romeo.\\n' # R\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
    "    'Nice meet you too. How are you today?\\n' # R\n",
    "    'Great. My baseball team won the competition.\\n' # J\n",
    "    'Oh Congratulations, Juliet\\n' # R\n",
    "    'Thank you Romeo\\n' # J\n",
    "    'Where are you going today?\\n' # R\n",
    "    'I am going shopping. What about you?\\n' # J\n",
    "    'I am going to visit my grandmother. she is not very well' # R\n",
    ")\n",
    "#正则表达式把。 ？去掉\n",
    "sentences = re.sub(\"[.,!?\\\\-]\",'',text.lower()).split('\\n')\n",
    "word_list = list(set(\" \".join(sentences).split())) # ['hello', 'how', 'are', 'you',...]\n",
    "#pad 是用来让句子的长度相同，cls放在开头，sep夹在两句话中间 mask随机替换单词\n",
    "word2idx = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\n",
    "for i,w in enumerate(word_list):\n",
    "    word2idx[w] = i + 4\n",
    "idx2word = {i:w for i,w in enumerate(word2idx)}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "#每个字都是一个token 下面这个数组存的是每一句话\n",
    "token_list =list()\n",
    "for sentence in sentences:\n",
    "    wordArray = [word2idx[s] for s in sentence.split()]\n",
    "    token_list.append(wordArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(token_list)\n",
    "#print(sentences)\n",
    "#randrange(len(sentences)) #输出 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里是模型的参数\n",
    "maxlen = 30 #所有样本的句子长度都一样， 这里我们把所有batch的样本句子的长度都一样\n",
    "batch_size = 6\n",
    "max_pred = 5 # max tokens of prediction 我们一句话里有0.15个token需要做mask \n",
    "n_layers = 6 #表示 Encoder Layer 的数量\n",
    "n_heads = 12 #multi head attention\n",
    "d_model = 768 #表示 Token Embeddings、Segment Embeddings、Position Embeddings 的维度\n",
    "d_ff = 768*4 # 4*d_model, FeedForward dimension 全连接神经网络得维度\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2 #一个样本是由多少句话构成的 在Bert的论文中是两句话构成的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面代码中，positive表示两句话是连续的，negative表示两句话不是连续的，我们需要做到在一个 batch 中，这两个样本的比例为 1:1。随机选取的两句话是否连续，只要通过判断 tokens_a_index + 1 == tokens_b_index 即可\n",
    "\n",
    "然后是随机 mask 一些 token，n_pred 变量代表的是即将 mask 的 token 数量，cand_maked_pos 代表的是有哪些位置是候选的、可以 mask 的（因为像 [SEP]，[CLS] 这些不能做 mask，没有意义），最后 shuffle() 一下，然后根据 random() 的值选择是替换为 [MASK] 还是替换为其它的 token\n",
    "\n",
    "接下来会做两个 Zero Padding，第一个是为了补齐句子的长度，使得一个 batch 中的句子都是相同长度。第二个是为了补齐 mask 的数量，因为不同句子长度，会导致不同数量的单词进行 mask，我们需要保证同一个 batch 中，mask 的数量（必须）是相同的，所以也需要在后面补一些没有意义的东西，比方说 [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下面我们要准备数据集了，这里是比较复杂的\n",
    "def make_data():\n",
    "    batch = []\n",
    "    positive = negative =0 #pos 表示这两句话是相邻的，neg表示两句话不是相邻的\n",
    "    #如果相邻 pos+1 不相邻 neg+1\n",
    "\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        #这一步是随机拼接两句话\n",
    "        #我们用tokens_a_index表示上一句话的索引，tokens_b_index表示下一句话的索引\n",
    "        #因为这里有九句话，我们就从这9句话随机抽取两句话的索引来作为输入\n",
    "        tokens_a_index,tokens_b_index =randrange(len(sentences)),randrange(len(sentences))\n",
    "        #取出这两句话中每一个token的索引\n",
    "        tokens_a,tokens_b = token_list[tokens_a_index],token_list[tokens_b_index]\n",
    "        #下面是处理每句话的输入\n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        #处理segment_ids 这里主要是让bert明白哪些是一句话\n",
    "        segment_ids = [0] * (1 + len(tokens_a) +1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "\n",
    "        #下面是mask掉几个输入 只有15%的token做mask\n",
    "        n_pred = min(max_pred, max(int(len(input_ids)*0.15),1))\n",
    "        #候选的mask的位置， 因为cls，sep不能做mask，\n",
    "        cand_maked_pos = [i for i,token in enumerate(input_ids)\n",
    "                                        if token !=[word2idx['[CLS]']] and token !=[word2idx['[SEP]']]]\n",
    "        #把侯选位置随机\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [],[]\n",
    "        #取前n_pred个\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            #把索引取出来\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            #bert有三种情况\n",
    "            if random()<0.8:\n",
    "                input_ids[pos] = word2idx['[MASK]']\n",
    "            elif random()>0.9:\n",
    "                #随机另外一个单词\n",
    "                index = randint(0, vocab_size-1)\n",
    "                while index < 4: #去掉无意义的字符\n",
    "                    index = randint(0, vocab_size-1)\n",
    "                input_ids[pos] = index\n",
    "        #做完这些 我们就要给这个句子补pad\n",
    "        n_pad =maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        #mask的个数也要相同\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "        \n",
    "        #判断这两句话是否是相邻的\n",
    "        #这里我们要使得neg和posi的数量是1:1\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids,segment_ids,masked_tokens,masked_pos,True]) #是下一个\n",
    "            positive += 1\n",
    "\n",
    "        if tokens_b_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids,segment_ids,masked_tokens,masked_pos,False]) #不是下一个\n",
    "            negative += 1\n",
    "     \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_data()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
    "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens),\\\n",
    "    torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "  def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
    "    self.input_ids = input_ids\n",
    "    self.segment_ids = segment_ids\n",
    "    self.masked_tokens = masked_tokens\n",
    "    self.masked_pos = masked_pos\n",
    "    self.isNext = isNext\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\n",
    "\n",
    "loader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext), batch_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#除掉句子中没用的字符\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    '''\n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    seq_len could be src_len or it could be tgt_len\n",
    "    seq_len in seq_q and seq_len in seq_k maybe not equal\n",
    "    '''\n",
    "    batch_size_q, len_q = seq_q.size()\n",
    "    # eq(zero) is PAD token\n",
    "    #每一个位置上的值和0比较，不是0就是F，是0就是True，再扩展一个维度，word_emb是三维的\n",
    "    pad_attn_mask = seq_q.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], False is masked\n",
    "    #batchsize 表示这里有几句话 这里encoder和decoder都会调用\n",
    "    return pad_attn_mask.expand(batch_size_q, len_q, len_q)  # [batch_size, len_q, len_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Q_V_2_U79Z_GHA64_P_G9DA.png](https://i.loli.net/2021/10/18/HbkJTOnMErejiL1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    #一个新的激活函数 bert论文中提出来的\n",
    "    \"\"\"Implementation of the gelu activation function.\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        Also see https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding,self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size,d_model)\n",
    "        self.pos_embed = nn.Embedding(maxlen,d_model)\n",
    "        self.seg_embed = nn.Embedding(n_segments,d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self,x,seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # [seq_len] -> [batch_size, seq_len]\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) +self.seg_embed(seg)\n",
    "        return self.norm(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''\n",
    "        Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K: [batch_size, n_heads, len_k, d_k]\n",
    "        V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        '''\n",
    "        #乘上k的转置 变成[lenq,lenk]\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, len_q, len_k]\n",
    "        #把attn_mask矩阵中为True的地方替换为-1e9\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.\n",
    "        \n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V) # [batch_size, n_heads, len_q, d_v]\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "        '''\n",
    "        input_Q: [batch_size, len_q, d_model]\n",
    "        input_K: [batch_size, len_k, d_model]\n",
    "        input_V: [batch_size, len_v(=len_k), d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        '''\n",
    "        residual, batch_size = input_Q, input_Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        #下面这一步我们给他做一个维度的变换，这是为了后面计算好算\n",
    "        q_s = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        k_s = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # K: [batch_size, n_heads, len_k, d_k]\n",
    "        v_s = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        #中间增加的维度扩城到n_heads个\n",
    "        #对于encoder这里的attn_mask就是去除的填充字符\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
    "\n",
    "        # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\n",
    "        context = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size, seq_len, n_heads * d_v]\n",
    "        output = self.fc(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual) # output: [batch_size, seq_len, d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#前馈神经网络 相比transformer这里的激活函数已经改变了\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
    "        return self.fc2(gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len, d_model]\n",
    "        enc_self_attn_mask: [batch_size, src_len, src_len]\n",
    "        '''\n",
    "        # enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]\n",
    "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, src_len, d_model]\n",
    "        return enc_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        #单词转换成输入向量的维度\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])#_ 是占位符， 表示不在意变量 的 值 只是用于循环遍历n次\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model,d_model),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model,2)\n",
    "        self.linear = nn.Linear(d_model,d_model)\n",
    "        self.activ2 = gelu\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.fc2.weight = embed_weight\n",
    "        \n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids) # [batch_size, seq_len, d_model]\n",
    "        #要把输入进去的多余字符p去掉，同时保证矩阵的大小\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids) # [batch_size, maxlen, maxlen]\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            # output: [batch_size, seq_len, d_model], \n",
    "            output = layer(output , enc_self_attn_mask)\n",
    "        #下面我们取出第一列CLS的token\n",
    "        h_pooled = self.fc(output[:,0])\n",
    "        #然后我们返回这一行的结果\n",
    "        logits_clsf = self.classifier(h_pooled)\n",
    "        \n",
    "        #下面这两行就是把预测位置的词往前移到对应位置，计算loss的时候正好可以一一对应。\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model) # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        \n",
    "        h_masked = self.activ2(self.linear(h_masked)) # [batch_size, max_pred, d_model]\n",
    "        logits_lm = self.fc2(h_masked) # [batch_size, max_pred, vocab_size]\n",
    "        return logits_lm, logits_clsf\n",
    "model = BERT()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss = 0.933714\n",
      "Epoch: 0020 loss = 0.780609\n",
      "Epoch: 0030 loss = 0.803110\n",
      "Epoch: 0040 loss = 0.769826\n",
      "Epoch: 0050 loss = 0.776828\n",
      "Epoch: 0060 loss = 0.842285\n",
      "Epoch: 0070 loss = 0.830547\n",
      "Epoch: 0080 loss = 0.759959\n",
      "Epoch: 0090 loss = 0.745616\n",
      "Epoch: 0100 loss = 0.746445\n",
      "Epoch: 0110 loss = 0.777969\n",
      "Epoch: 0120 loss = 0.726676\n",
      "Epoch: 0130 loss = 0.752499\n",
      "Epoch: 0140 loss = 0.779684\n",
      "Epoch: 0150 loss = 0.723779\n",
      "Epoch: 0160 loss = 0.764170\n",
      "Epoch: 0170 loss = 0.645281\n",
      "Epoch: 0180 loss = 0.735849\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(180):\n",
    "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in loader:\n",
    "      logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "      #下面这一行主要实现了将[batch_size, max_pred, d_model]转换为[batch_size * max_pred, d_model]\n",
    "      #这里一个batch是一句话\n",
    "      loss_lm = criterion(logits_lm.view(-1, vocab_size), masked_tokens.view(-1)) # for masked LM\n",
    "      loss_lm = (loss_lm.float()).mean()\n",
    "      loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
    "      loss = loss_lm + loss_clsf\n",
    "      if (epoch + 1) % 10 == 0:\n",
    "          print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I am Romeo.\n",
      "Hello, Romeo My name is Juliet. Nice to meet you.\n",
      "Nice meet you too. How are you today?\n",
      "Great. My baseball team won the competition.\n",
      "Oh Congratulations, Juliet\n",
      "Thank you Romeo\n",
      "Where are you going today?\n",
      "I am going shopping. What about you?\n",
      "I am going to visit my grandmother. she is not very well\n",
      "['[CLS]', '[MASK]', 'meet', 'you', 'too', '[MASK]', 'are', 'you', 'today', '[SEP]', 'great', 'my', 'baseball', 'team', 'won', 'the', 'competition', '[SEP]']\n",
      "masked tokens list :  [37, 39]\n",
      "predict masked tokens list :  [37, 39]\n",
      "isNext :  True\n",
      "predict isNext :  True\n"
     ]
    }
   ],
   "source": [
    "# Predict mask tokens ans isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[5]\n",
    "print(text)\n",
    "print([idx2word[w] for w in input_ids if idx2word[w] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(torch.LongTensor([input_ids]), \\\n",
    "                 torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('masked tokens list : ',[pos for pos in masked_tokens if pos != 0])\n",
    "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_clsf else False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "748dbdca4f5d9507dbf5438deb9fb0c5af4d959a1362599ffdf2eaf1f99424f2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
